{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c32c924c",
   "metadata": {},
   "source": [
    "## Off the shelf results with T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "928f6d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d883b7d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "base_tokenizer = T5Tokenizer.from_pretrained('t5-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c21f6b",
   "metadata": {},
   "source": [
    "## Abstractive Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd740019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text preprocessed:\n",
      " Sinan Ozdemir is a data scientist, startup founder, and educator living in the San Francisco Bay Area with his dog, Charlie; cat, Euclid; and bearded dragon, Fiero. He spent his academic career studying pure mathematics at Johns Hopkins University before transitioning to education. He spent several years conducting lectures on data science at Johns Hopkins University and at the General Assembly before founding his own startup, Kylie.ai, which uses artificial intelligence to build chatbots from historical transcripts. After completing a Fellowship at the Y Combinator accelerator, Sinan spent most of his time working on his fast-growing company, while creating educational material for data science.\n"
     ]
    }
   ],
   "source": [
    "text_to_summarize =\"\"\"Sinan Ozdemir is a data scientist, startup founder, and educator living in the San Francisco Bay Area with his dog, \n",
    "Charlie; cat, Euclid; and bearded dragon, Fiero. He spent his academic career studying pure mathematics \n",
    "at Johns Hopkins University before transitioning to education. He spent several years conducting lectures \n",
    "on data science at Johns Hopkins University and at the General Assembly before founding his own startup, \n",
    "Kylie.ai, which uses artificial intelligence to build chatbots from historical transcripts. \n",
    "After completing a Fellowship at the Y Combinator accelerator, Sinan spent most of his time working on \n",
    "his fast-growing company, while creating educational material for data science.\n",
    "\"\"\"\n",
    "\n",
    "preprocess_text = text_to_summarize.strip().replace(\"\\n\",\"\")\n",
    "\n",
    "print (\"original text preprocessed:\\n\", preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b987bdf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized text: \n",
      "Sinan Ozdemir is a data scientist, startup founder, and educator. he founded his own startup, Kylie.ai, which uses artificial intelligence to build chatbots.\n"
     ]
    }
   ],
   "source": [
    "# known prompt for summarization with T5\n",
    "t5_prepared_text = \"summarize: \" + preprocess_text\n",
    "\n",
    "input_ids = base_tokenizer.encode(t5_prepared_text, return_tensors=\"pt\")\n",
    "\n",
    "# summmarize \n",
    "summary_ids = base_model.generate(\n",
    "    input_ids,\n",
    "    num_beams=4,\n",
    "    no_repeat_ngram_size=3,\n",
    "    min_length=30,\n",
    "    max_length=50,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "output = base_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print (f\"Summarized text: \\n{output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d69bd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cba2c628",
   "metadata": {},
   "source": [
    "## English -> German Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97f956b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated text:\n",
      "Wo ist die Schokolade?\n"
     ]
    }
   ],
   "source": [
    "input_ids = base_tokenizer.encode('translate English to German: Where is the chocolate?', return_tensors='pt')\n",
    "\n",
    "# translate \n",
    "translate_ids = base_model.generate(\n",
    "    input_ids,\n",
    "    num_beams=4,\n",
    "    no_repeat_ngram_size=3,\n",
    "    max_length=20,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "output = base_tokenizer.decode(translate_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print (f\"Translated text:\\n{output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6bb64ef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 3488,   229,    67, 31267,    58,     1]]),\n",
       " tensor(0.1136, grad_fn=<NllLossBackward0>))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pass labels in to calculate loss\n",
    "\n",
    "input_ids = base_tokenizer('translate English to German: Where is the chocolate?', return_tensors='pt').input_ids\n",
    "labels = base_tokenizer('Wo ist die Schokolade?', return_tensors='pt').input_ids\n",
    "\n",
    "loss = base_model(input_ids=input_ids, labels=labels).loss\n",
    "\n",
    "labels, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0397d662",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3012d5e3",
   "metadata": {},
   "source": [
    "## CoLA: The Corpus of Linguistic Acceptability\n",
    "checking for grammatical correctess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ada39336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is grammatically correct?: \n",
      "acceptable\n"
     ]
    }
   ],
   "source": [
    "input_ids = base_tokenizer.encode('cola sentence: Where is the chocolate?', return_tensors='pt')\n",
    "\n",
    "# CoLA \n",
    "translate_ids = base_model.generate(\n",
    "    input_ids,\n",
    "    num_beams=4,\n",
    "    no_repeat_ngram_size=3,\n",
    "    max_length=20,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "output = base_tokenizer.decode(translate_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"is grammatically correct?: \\n{output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2200dc26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is grammatically correct?: \n",
      "unacceptable\n"
     ]
    }
   ],
   "source": [
    "input_ids = base_tokenizer.encode('cola sentence: Where be a chocolates?', return_tensors='pt')\n",
    "\n",
    "# summmarize \n",
    "translate_ids = base_model.generate(\n",
    "    input_ids,\n",
    "    max_length=20,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "output = base_tokenizer.decode(translate_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"is grammatically correct?: \\n{output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb731953",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "509b5967",
   "metadata": {},
   "source": [
    "## STSB - Semantic Text Similarity Benchmark\n",
    "Are two sentences semantically similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff07d3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "semantically similar? (1-5): \n",
      "3.2\n"
     ]
    }
   ],
   "source": [
    "sentence_one = 'How to fish'\n",
    "sentence_two = 'Fishing Manual for beginnners'\n",
    "\n",
    "\n",
    "input_ids = base_tokenizer.encode(f\"stsb sentence1: {sentence_one} sentence2: {sentence_two}\", return_tensors='pt')\n",
    "\n",
    "# calculate semantic similarity \n",
    "translate_ids = base_model.generate(\n",
    "    input_ids,\n",
    "    max_length=3,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "output = base_tokenizer.decode(translate_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"semantically similar? (0-5): \\n{output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b361b234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "semantically similar? (1-5): \n",
      "0.4\n"
     ]
    }
   ],
   "source": [
    "sentence_one = 'How to fish'\n",
    "sentence_two = 'Hiking Manual for beginnners'\n",
    "\n",
    "\n",
    "input_ids = base_tokenizer.encode(f\"stsb sentence1: {sentence_one} sentence2: {sentence_two}\", return_tensors='pt')\n",
    "\n",
    "# calculate semantic similarity\n",
    "translate_ids = base_model.generate(\n",
    "    input_ids,\n",
    "    max_length=3,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "output = base_tokenizer.decode(translate_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"semantically similar? (0-5): \\n{output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ea1065",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3081d51",
   "metadata": {},
   "source": [
    "## MNLI - Multi-Genre Natural Language Inference\n",
    "Whether a premise implies (“entailment”), contradicts (“contradiction”), or neither (“neutral”) a hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3808dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: \n",
      "entailment\n"
     ]
    }
   ],
   "source": [
    "input_ids = base_tokenizer.encode(\n",
    "    'mnli premise: I am active in politics. hypothesis: I am running for mayor', return_tensors='pt'\n",
    ")\n",
    "\n",
    "# mnli \n",
    "translate_ids = base_model.generate(\n",
    "    input_ids,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "output = base_tokenizer.decode(translate_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Response: \\n{output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4054466d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: \n",
      "contradiction\n"
     ]
    }
   ],
   "source": [
    "input_ids = base_tokenizer.encode(\n",
    "    'mnli premise: I am active in politics. hypothesis: I do not really vote', return_tensors='pt'\n",
    ")\n",
    "\n",
    "# mnli \n",
    "translate_ids = base_model.generate(\n",
    "    input_ids,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "output = base_tokenizer.decode(translate_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Response: \\n{output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab54bbec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: \n",
      "neutral\n"
     ]
    }
   ],
   "source": [
    "input_ids = base_tokenizer.encode(\n",
    "    'mnli premise: I am active in politics. hypothesis: I code for a living', return_tensors='pt'\n",
    ")\n",
    "\n",
    "# mnli \n",
    "translate_ids = base_model.generate(\n",
    "    input_ids,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "output = base_tokenizer.decode(translate_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Response: \\n{output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49aecb68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca923c09",
   "metadata": {},
   "source": [
    "## Q/A - Question/Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "31f2923f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: \n",
      "California\n"
     ]
    }
   ],
   "source": [
    "input_ids = base_tokenizer.encode(\n",
    "    'question: Where does Sinan live? context: Sinan lives in California but Matt lives in Boston.', return_tensors='pt'\n",
    ")\n",
    "\n",
    "# Q/A\n",
    "translate_ids = base_model.generate(\n",
    "    input_ids,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "output = base_tokenizer.decode(translate_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Response: \\n{output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4845657",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b0cd275e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: \n",
      "Boston\n"
     ]
    }
   ],
   "source": [
    "input_ids = base_tokenizer.encode(\n",
    "    'question: Where does Matt live? context: Sinan lives in California but Matt lives in Boston.', return_tensors='pt'\n",
    ")\n",
    "\n",
    "# Q/A\n",
    "translate_ids = base_model.generate(\n",
    "    input_ids,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "output = base_tokenizer.decode(translate_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Response: \\n{output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2d4fee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c20b4ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: \n",
      "prompt2\n"
     ]
    }
   ],
   "source": [
    "# Sanity check with random prompts\n",
    "\n",
    "input_ids = base_tokenizer.encode(\n",
    "    'prompt1: Where does Matt live? prompt2: Sinan lives in California but Matt lives in Boston.', return_tensors='pt'\n",
    ")\n",
    "\n",
    "# Q/A\n",
    "translate_ids = base_model.generate(\n",
    "    input_ids,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "output = base_tokenizer.decode(translate_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Response: \\n{output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5913ab77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21434974",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "336c91b9",
   "metadata": {},
   "source": [
    "## Fine-tuning T5 for abstractive summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8f4fcdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, T5ForConditionalGeneration, TrainingArguments, Trainer, \\\n",
    "                         DataCollatorForSeq2Seq\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ceea57f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "base_tokenizer = T5Tokenizer.from_pretrained('t5-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "33f3a084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96486, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "      <td>Great taffy.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This taffy is so good.  It is very soft and ch...</td>\n",
       "      <td>Wonderful, tasty taffy.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Right now I'm mostly just sprouting this so my...</td>\n",
       "      <td>Yay Barley.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is a very healthy dog food. Good for thei...</td>\n",
       "      <td>Healthy Dog Food.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>good flavor! these came securely packed... the...</td>\n",
       "      <td>fresh and greasy!</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text                  Summary  \\\n",
       "0  Great taffy at a great price.  There was a wid...             Great taffy.   \n",
       "1  This taffy is so good.  It is very soft and ch...  Wonderful, tasty taffy.   \n",
       "2  Right now I'm mostly just sprouting this so my...              Yay Barley.   \n",
       "3  This is a very healthy dog food. Good for thei...        Healthy Dog Food.   \n",
       "4  good flavor! these came securely packed... the...        fresh and greasy!   \n",
       "\n",
       "   Score  \n",
       "0      5  \n",
       "1      5  \n",
       "2      5  \n",
       "3      5  \n",
       "4      4  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://www.kaggle.com/snap/amazon-fine-food-reviews?select=Reviews.csv\n",
    "\n",
    "reviews = pd.read_csv('../data/reviews.csv')\n",
    "\n",
    "# Pre-processing step\n",
    "# Punctuation is important in grammar and important for complex decoding architectures to know when to stop!\n",
    "def add_punc(s):\n",
    "    if s[-1] not in ('.', '!', '?'):\n",
    "        s = s + '.'\n",
    "    return s\n",
    "\n",
    "reviews.dropna(inplace=True)\n",
    "\n",
    "reviews['Summary'] = reviews['Summary'].map(add_punc)\n",
    "\n",
    "print(reviews.shape)\n",
    "\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "da4c1dcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13073, 3)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = reviews[(reviews['Summary'].str.len() < 100) & (reviews['Summary'].str.len() >=30)]\n",
    "\n",
    "reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "41ba36f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "\n",
    "reviews_dataset = Dataset.from_pandas(reviews.astype(str).sample(5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6e4aff70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have a prompt but only as a prefix in the encoder\n",
    "prefix = \"summarize: \"\n",
    "\n",
    "# we will manually add our own labels because unlike GPT, we cannot assume the labels are based on the inputs\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"Text\"]]\n",
    "    model_inputs = base_tokenizer(inputs, max_length=1024, truncation=True)\n",
    "\n",
    "    labels = base_tokenizer(examples[\"Summary\"], max_length=128, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78ae0a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "822208b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c1486b6a831452aaa1931dd58f3c979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "tokenized_reviews_dataset = reviews_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28641597",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e71d418e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_reviews_dataset = tokenized_reviews_dataset.train_test_split(test_size=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ca5565f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator specifically for generic sequence to sequence tasks\n",
    "# Use when we are translating one sequence to another like translation, summarization, etc\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=base_tokenizer, model=base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "dd24d2d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: Score, __index_level_0__, Summary, Text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 27:32]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 3.532301187515259,\n",
       " 'eval_runtime': 38.495,\n",
       " 'eval_samples_per_second': 12.989,\n",
       " 'eval_steps_per_second': 0.416}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./t5_summary_results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=20,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=50,\n",
    "    save_strategy='epoch'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=base_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_reviews_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_reviews_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2f1671d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: Score, __index_level_0__, Summary, Text.\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_env/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4500\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2820\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2820' max='2820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2820/2820 8:57:51, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.563800</td>\n",
       "      <td>3.173784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.391200</td>\n",
       "      <td>3.113433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.289800</td>\n",
       "      <td>3.072913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.288000</td>\n",
       "      <td>3.041173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.176900</td>\n",
       "      <td>3.020081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.187400</td>\n",
       "      <td>2.997320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.112700</td>\n",
       "      <td>2.977490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.047000</td>\n",
       "      <td>2.964586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.034600</td>\n",
       "      <td>2.953549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.034000</td>\n",
       "      <td>2.941815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>3.024300</td>\n",
       "      <td>2.933490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.912400</td>\n",
       "      <td>2.922354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.960400</td>\n",
       "      <td>2.919024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.921700</td>\n",
       "      <td>2.913339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.919800</td>\n",
       "      <td>2.908537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.926800</td>\n",
       "      <td>2.906170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.893600</td>\n",
       "      <td>2.903067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.899000</td>\n",
       "      <td>2.900889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.867900</td>\n",
       "      <td>2.899698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.879100</td>\n",
       "      <td>2.899170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: Score, __index_level_0__, Summary, Text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5_summary_results/checkpoint-141\n",
      "Configuration saved in ./t5_summary_results/checkpoint-141/config.json\n",
      "Model weights saved in ./t5_summary_results/checkpoint-141/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5_summary_results/checkpoint-141/tokenizer_config.json\n",
      "Special tokens file saved in ./t5_summary_results/checkpoint-141/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: Score, __index_level_0__, Summary, Text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5_summary_results/checkpoint-282\n",
      "Configuration saved in ./t5_summary_results/checkpoint-282/config.json\n",
      "Model weights saved in ./t5_summary_results/checkpoint-282/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5_summary_results/checkpoint-282/tokenizer_config.json\n",
      "Special tokens file saved in ./t5_summary_results/checkpoint-282/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: Score, __index_level_0__, Summary, Text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5_summary_results/checkpoint-423\n",
      "Configuration saved in ./t5_summary_results/checkpoint-423/config.json\n",
      "Model weights saved in ./t5_summary_results/checkpoint-423/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5_summary_results/checkpoint-423/tokenizer_config.json\n",
      "Special tokens file saved in ./t5_summary_results/checkpoint-423/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: Score, __index_level_0__, Summary, Text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5_summary_results/checkpoint-564\n",
      "Configuration saved in ./t5_summary_results/checkpoint-564/config.json\n",
      "Model weights saved in ./t5_summary_results/checkpoint-564/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5_summary_results/checkpoint-564/tokenizer_config.json\n",
      "Special tokens file saved in ./t5_summary_results/checkpoint-564/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: Score, __index_level_0__, Summary, Text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5_summary_results/checkpoint-705\n",
      "Configuration saved in ./t5_summary_results/checkpoint-705/config.json\n",
      "Model weights saved in ./t5_summary_results/checkpoint-705/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5_summary_results/checkpoint-705/tokenizer_config.json\n",
      "Special tokens file saved in ./t5_summary_results/checkpoint-705/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: Score, __index_level_0__, Summary, Text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5_summary_results/checkpoint-846\n",
      "Configuration saved in ./t5_summary_results/checkpoint-846/config.json\n",
      "Model weights saved in ./t5_summary_results/checkpoint-846/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5_summary_results/checkpoint-846/tokenizer_config.json\n",
      "Special tokens file saved in ./t5_summary_results/checkpoint-846/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: Score, __index_level_0__, Summary, Text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5_summary_results/checkpoint-987\n",
      "Configuration saved in ./t5_summary_results/checkpoint-987/config.json\n",
      "Model weights saved in ./t5_summary_results/checkpoint-987/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5_summary_results/checkpoint-987/tokenizer_config.json\n",
      "Special tokens file saved in ./t5_summary_results/checkpoint-987/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: Score, __index_level_0__, Summary, Text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5_summary_results/checkpoint-1128\n",
      "Configuration saved in ./t5_summary_results/checkpoint-1128/config.json\n",
      "Model weights saved in ./t5_summary_results/checkpoint-1128/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5_summary_results/checkpoint-1128/tokenizer_config.json\n",
      "Special tokens file saved in ./t5_summary_results/checkpoint-1128/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: Score, __index_level_0__, Summary, Text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5_summary_results/checkpoint-1269\n",
      "Configuration saved in ./t5_summary_results/checkpoint-1269/config.json\n",
      "Model weights saved in ./t5_summary_results/checkpoint-1269/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5_summary_results/checkpoint-1269/tokenizer_config.json\n",
      "Special tokens file saved in ./t5_summary_results/checkpoint-1269/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: Score, __index_level_0__, Summary, Text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5_summary_results/checkpoint-1410\n",
      "Configuration saved in ./t5_summary_results/checkpoint-1410/config.json\n",
      "Model weights saved in ./t5_summary_results/checkpoint-1410/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5_summary_results/checkpoint-1410/tokenizer_config.json\n",
      "Special tokens file saved in ./t5_summary_results/checkpoint-1410/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: Score, __index_level_0__, Summary, Text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5_summary_results/checkpoint-1551\n",
      "Configuration saved in ./t5_summary_results/checkpoint-1551/config.json\n",
      "Model weights saved in ./t5_summary_results/checkpoint-1551/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5_summary_results/checkpoint-1551/tokenizer_config.json\n",
      "Special tokens file saved in ./t5_summary_results/checkpoint-1551/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: Score, __index_level_0__, Summary, Text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5_summary_results/checkpoint-1692\n",
      "Configuration saved in ./t5_summary_results/checkpoint-1692/config.json\n",
      "Model weights saved in ./t5_summary_results/checkpoint-1692/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5_summary_results/checkpoint-1692/tokenizer_config.json\n",
      "Special tokens file saved in ./t5_summary_results/checkpoint-1692/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: Score, __index_level_0__, Summary, Text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5_summary_results/checkpoint-1833\n",
      "Configuration saved in ./t5_summary_results/checkpoint-1833/config.json\n",
      "Model weights saved in ./t5_summary_results/checkpoint-1833/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in ./t5_summary_results/checkpoint-1833/tokenizer_config.json\n",
      "Special tokens file saved in ./t5_summary_results/checkpoint-1833/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: Score, __index_level_0__, Summary, Text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5_summary_results/checkpoint-1974\n",
      "Configuration saved in ./t5_summary_results/checkpoint-1974/config.json\n",
      "Model weights saved in ./t5_summary_results/checkpoint-1974/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5_summary_results/checkpoint-1974/tokenizer_config.json\n",
      "Special tokens file saved in ./t5_summary_results/checkpoint-1974/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: Score, __index_level_0__, Summary, Text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5_summary_results/checkpoint-2115\n",
      "Configuration saved in ./t5_summary_results/checkpoint-2115/config.json\n",
      "Model weights saved in ./t5_summary_results/checkpoint-2115/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5_summary_results/checkpoint-2115/tokenizer_config.json\n",
      "Special tokens file saved in ./t5_summary_results/checkpoint-2115/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: Score, __index_level_0__, Summary, Text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5_summary_results/checkpoint-2256\n",
      "Configuration saved in ./t5_summary_results/checkpoint-2256/config.json\n",
      "Model weights saved in ./t5_summary_results/checkpoint-2256/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5_summary_results/checkpoint-2256/tokenizer_config.json\n",
      "Special tokens file saved in ./t5_summary_results/checkpoint-2256/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: Score, __index_level_0__, Summary, Text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5_summary_results/checkpoint-2397\n",
      "Configuration saved in ./t5_summary_results/checkpoint-2397/config.json\n",
      "Model weights saved in ./t5_summary_results/checkpoint-2397/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5_summary_results/checkpoint-2397/tokenizer_config.json\n",
      "Special tokens file saved in ./t5_summary_results/checkpoint-2397/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: Score, __index_level_0__, Summary, Text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5_summary_results/checkpoint-2538\n",
      "Configuration saved in ./t5_summary_results/checkpoint-2538/config.json\n",
      "Model weights saved in ./t5_summary_results/checkpoint-2538/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5_summary_results/checkpoint-2538/tokenizer_config.json\n",
      "Special tokens file saved in ./t5_summary_results/checkpoint-2538/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: Score, __index_level_0__, Summary, Text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5_summary_results/checkpoint-2679\n",
      "Configuration saved in ./t5_summary_results/checkpoint-2679/config.json\n",
      "Model weights saved in ./t5_summary_results/checkpoint-2679/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5_summary_results/checkpoint-2679/tokenizer_config.json\n",
      "Special tokens file saved in ./t5_summary_results/checkpoint-2679/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: Score, __index_level_0__, Summary, Text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./t5_summary_results/checkpoint-2820\n",
      "Configuration saved in ./t5_summary_results/checkpoint-2820/config.json\n",
      "Model weights saved in ./t5_summary_results/checkpoint-2820/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5_summary_results/checkpoint-2820/tokenizer_config.json\n",
      "Special tokens file saved in ./t5_summary_results/checkpoint-2820/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./t5_summary_results/checkpoint-2820 (score: 2.899169921875).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2820, training_loss=3.0722037714423864, metrics={'train_runtime': 32279.8447, 'train_samples_per_second': 2.788, 'train_steps_per_second': 0.087, 'total_flos': 1246064474849280.0, 'train_loss': 3.0722037714423864, 'epoch': 20.0})"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()  # total of 9 hours of training on my laptop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "54510733",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: Score, __index_level_0__, Summary, Text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 00:34]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.899169921875,\n",
       " 'eval_runtime': 37.0292,\n",
       " 'eval_samples_per_second': 13.503,\n",
       " 'eval_steps_per_second': 0.432,\n",
       " 'epoch': 20.0}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "221a71f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./t5_summary_results\n",
      "Configuration saved in ./t5_summary_results/config.json\n",
      "Model weights saved in ./t5_summary_results/pytorch_model.bin\n",
      "tokenizer config file saved in ./t5_summary_results/tokenizer_config.json\n",
      "Special tokens file saved in ./t5_summary_results/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6ab1d16d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loaded_model = T5ForConditionalGeneration.from_pretrained('./t5_summary_results')\n",
    "\n",
    "# summarization pipeline prepends a default prefix of summarize: \n",
    "generator = pipeline(\n",
    "    'summarization', model=loaded_model, tokenizer=base_tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ad6b8780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79402    amazing, my 11 year old lab, can play with my ...\n",
      "Name: Summary, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'my 11 year old lab can once again run and play ball with my freinds. she  loves them. it like a wonder treat!!'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sam = reviews.sample(1)\n",
    "\n",
    "print(sam['Summary'])\n",
    "\n",
    "text = sam['Text'].tolist()[0]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c325f6c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'my 11 year old lab can play with my freinds again!!'}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a summary\n",
    "generator(text, min_length=3, max_length=15, early_stopping=True, num_beams=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c7d83a20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'my lab can run and play ball with my freinds .'}]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try the base t5 on the same text\n",
    "base_generator = pipeline(\n",
    "    'summarization', model='t5-small', tokenizer='t5-small'\n",
    ")\n",
    "\n",
    "# Summary is a bit more extractive than our fine-tuned version and style isn't quite the same as our dataset\n",
    "base_generator(text, min_length=3, max_length=15, early_stopping=True, num_beams=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02031ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
